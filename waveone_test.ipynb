{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "waveone-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caoscott/waveone/blob/master/waveone_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s61AjCA2mjMQ",
        "outputId": "8911a88c-7c3e-434b-fc6f-dcef1dc58e64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!nvidia-smi\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Nov 12 02:23:13 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    88W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73WEapNa9yNN",
        "colab_type": "code",
        "outputId": "fba9a3f3-4d8e-4e4e-a63a-f362514d4981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-12 02:23:15--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.71.61.108, 52.20.12.96, 3.82.169.244, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.71.61.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.14’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M   440KB/s    in 18s     \n",
            "\n",
            "2019-11-12 02:23:34 (735 KB/s) - ‘ngrok-stable-linux-amd64.zip.14’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GRXWeBjhnfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir . --host 0.0.0.0 --port 6006 &'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igrQ5PwIiDsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaPk4nQqiEQi",
        "colab_type": "code",
        "outputId": "f28572ec-c158-4521-cf6b-caff9cd59d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        }
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://76303530.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kKsf10uzcN9P",
        "outputId": "ac1d7107-a290-4a83-9840-4fe95a2ba76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1ddmjU48TehTk28903cg2mSoLBgkeAcwY\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1ddmjU48TehTk28903cg2mSoLBgkeAcwY\" -o vcii_demo_data.tar.gz > /dev/null"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   388    0   388    0     0   1596      0 --:--:-- --:--:-- --:--:--  1596\n",
            "100  104M    0  104M    0     0  89.0M      0 --:--:--  0:00:01 --:--:--  197M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqR6ErSzYX1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xvf vcii_demo_data.tar.gz &> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6TOlFRhb85Am",
        "outputId": "afbd1e29-2f8e-449f-d9c1-89f5888ce375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "!rm -rf waveone\n",
        "!git clone https://github.com/caoscott/waveone.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'waveone'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 688 (delta 59), reused 67 (delta 30), pack-reused 591\u001b[K\n",
            "Receiving objects: 100% (688/688), 113.53 KiB | 334.00 KiB/s, done.\n",
            "Resolving deltas: 100% (454/454), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kPZ01eOWrol7",
        "outputId": "b193d43d-a175-4d5a-a167-360fd685869f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "# !cd waveone && python3 -u train.py \\\n",
        "#   --train ../data/train \\\n",
        "#   --eval ../data/eval \\\n",
        "#   --save-out-img \\\n",
        "#   --lr 0.001 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --eval-batch-size 1 \\\n",
        "#   --patch 64 \\\n",
        "#   --bits 1024 \\\n",
        "#   --binarize-off\n",
        "from waveone.train_options import parser\n",
        "from waveone.train import train\n",
        "\n",
        "nets = None\n",
        "args = parser.parse_args([\n",
        "    \"--train\", \"data/train\",\n",
        "    \"--eval\", \"data/eval\",\n",
        "    \"--lr\", \"0.001\",\n",
        "    \"--batch-size\", \"16\",\n",
        "    \"--eval-batch-size\", \"1\",\n",
        "    \"--patch\", \"128\",\n",
        "    \"--bits\", \"2048\",\n",
        "    \"--binarize-off\",\n",
        "    \"--max-train-epochs\", \"100\",\n",
        "    \"--save-out-img\",\n",
        "])\n",
        "print(args)\n",
        "nets = train(args)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=16, binarize_off=True, bits=2048, checkpoint_epochs=100, clip=0.5, eval='data/eval', eval_batch_size=1, eval_epochs=10, gamma=0.5, gpus='0', load_iter=None, load_model_name=None, lr=0.001, max_train_epochs=100, model_dir='model', out_dir='output', patch=128, save_codes=False, save_model_name='demo', save_out_img=True, schedule='50000,60000,70000,80000,90000', train='data/train', weight_decay=0.0005)\n",
            "679 images loaded.\n",
            "Loader for 677 images (42 batches) created.\n",
            "13 images loaded.\n",
            "Loader for 13 images (13 batches) created.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2751: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhAsNdjg5L16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from waveone.dataset import get_loader\n",
        "\n",
        "eval_loader = get_loader(\n",
        "    is_train=False,\n",
        "    root=args.eval,\n",
        "    frame_len=1,\n",
        "    sampling_range=0,\n",
        "    args=args,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKkCyx5hjVyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from waveone.losses import MSSSIM\n",
        "\n",
        "msssim_fn = MSSSIM(val_range=1, normalize=True).cuda()\n",
        "# charbonnier_loss_fn = CharbonnierLoss().cuda()\n",
        "l1_loss_fn = nn.L1Loss(reduction=\"mean\").cuda()\n",
        "\n",
        "def eval_scores(\n",
        "    frames1: torch.Tensor,\n",
        "    frames2: torch.Tensor,\n",
        "    prefix: str\n",
        "):\n",
        "    assert len(frames1) == len(frames2)\n",
        "    frame_len = len(frames1)\n",
        "    msssim = 0.\n",
        "    l1 = 0.\n",
        "    for frame1, frame2 in zip(frames1, frames2):\n",
        "        l1 += l1_loss_fn(frame1, frame2)\n",
        "        msssim += msssim_fn(frame1, frame2)\n",
        "    return {f\"{prefix}_l1\": l1/frame_len,\n",
        "            f\"{prefix}_msssim\": msssim/frame_len}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1TLotbMaEXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "context_vec_test_shape = (args.eval_batch_size, 512, 144, 176)\n",
        "\n",
        "for net in nets:\n",
        "    net.eval()\n",
        "\n",
        "encoder, binarizer, decoder = nets\n",
        "\n",
        "with torch.no_grad():\n",
        "    context_vec = torch.zeros(context_vec_test_shape).cuda()\n",
        "    total_scores = defaultdict(float)\n",
        "    frame1 = None\n",
        "\n",
        "    for frame2, in eval_loader:\n",
        "        frame2 = frame2.cuda()\n",
        "        if frame1 is None:\n",
        "            frame1 = frame2\n",
        "        else:\n",
        "            codes = binarizer(encoder(frame1, frame2, context_vec))\n",
        "            flows, residuals, context_vec = decoder((codes, context_vec))\n",
        "            flow_frame2 = F.grid_sample(frame1, flows)\n",
        "            reconstructed_frame2 = (\n",
        "                flow_frame2 + residuals).clamp(-0.5, 0.5)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow((frame2 + 0.5).cpu().squeeze().permute(1, 2, 0))\n",
        "            plt.figure()\n",
        "            plt.imshow((reconstructed_frame2 + 0.5).cpu().squeeze().permute(1, 2, 0))\n",
        "\n",
        "            total_scores = {\n",
        "                **eval_scores([frame1], [frame2], \"eval_baseline\"),\n",
        "                **eval_scores([frame2], [flow_frame2], \"eval_flow\"),\n",
        "                **eval_scores([frame2], [reconstructed_frame2], \"eval_reconstructed\"),\n",
        "            }\n",
        "\n",
        "            # Update frame1.\n",
        "            frame1 = reconstructed_frame2\n",
        "\n",
        "    total_scores = {k: v.item()/len(eval_loader.dataset)\n",
        "                    for k, v in total_scores.items()}\n",
        "    print(total_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te9HoPAeEDYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "img = plt.imread(\"pytorch-vcii/output/iter100/images/silent_cif_0012.png_iter10.png\")\n",
        "plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZjTLmDBTpBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def _generate_indices(index, length, times):\n",
        "#     indices = []\n",
        "#     for _ in range(times):\n",
        "#         indices.append(index % length)\n",
        "#         index //= length\n",
        "#     return tuple(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79sOKR1zTuMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# length = 3\n",
        "# times = 4\n",
        "# l = set([_generate_indices(i, length, times) for i in range(length ** times)])\n",
        "# len(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ4uwPi-219k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bUqFDv3s4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !cp -r waveone/ \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "283-kdgT8BLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}